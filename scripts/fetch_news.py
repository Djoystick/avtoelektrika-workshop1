#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import feedparser
import json
import sys
import os
import re
import html
from datetime import datetime

# –ü–æ–¥–∫–ª—é—á–∞–µ–º –∫–æ–Ω—Ñ–∏–≥
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
from config import (
    NEWS_SOURCES, MAX_NEWS_PER_SOURCE, MAX_TOTAL_NEWS,
    EXCLUDE_KEYWORDS, TECH_KEYWORDS, SYMPTOMS_KEYWORDS, OUTPUT_FILE
)

# === –ú–ê–°–ö–ò–†–û–í–ö–ê –ü–û–î –ë–†–ê–£–ó–ï–† ===
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'
}

def is_technical(title, summary):
    text = (title + " " + summary).lower()
    # –°—Ç—Ä–æ–≥–∏–π —Ñ–∏–ª—å—Ç—Ä —Å—Ç–æ–ø-—Å–ª–æ–≤
    for keyword in EXCLUDE_KEYWORDS:
        if keyword.lower() in text:
            return False
    # –ú—è–≥–∫–∏–π —Ñ–∏–ª—å—Ç—Ä: –µ—Å–ª–∏ –µ—Å—Ç—å —Ç–µ—Ö. —Å–ª–æ–≤–∞ - –±–µ—Ä–µ–º
    for keyword in TECH_KEYWORDS:
        if keyword.lower() in text:
            return True
    return False

def extract_symptoms(title, summary):
    text = (title + " " + summary).lower()
    found = [s for s in SYMPTOMS_KEYWORDS if s.lower() in text]
    return list(set(found))[:8]

def extract_image(entry):
    # 1. YouTube (–≤—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ)
    if 'youtube.com' in entry.get('link', ''):
        if 'media_group' in entry and 'media_thumbnail' in entry.media_group[0]:
            return entry.media_group[0]['media_thumbnail'][0]['url']
    
    # 2. –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –ø–æ–ª—è RSS
    if hasattr(entry, 'media_content') and entry.media_content:
        return entry.media_content[0].get('url', '')
    if hasattr(entry, 'media_thumbnail') and entry.media_thumbnail:
        return entry.media_thumbnail[0].get('url', '')
    if hasattr(entry, 'enclosures') and entry.enclosures:
        for enc in entry.enclosures:
            if enc.type.startswith('image/'):
                return enc.href

    # 3. –ü–æ–∏—Å–∫ –∫–∞—Ä—Ç–∏–Ω–∫–∏ –≤–Ω—É—Ç—Ä–∏ HTML —Ç–µ–∫—Å—Ç–∞
    if 'summary' in entry:
        match = re.search(r'<img[^>]*src=["\'](.*?)["\']', entry.summary)
        if match:
            return match.group(1)
    if 'content' in entry:
        for c in entry.content:
            match = re.search(r'<img[^>]*src=["\'](.*?)["\']', c.value)
            if match:
                return match.group(1)
                
    return None

def clean_html(text):
    if not text: return ''
    
    # 1. –î–µ–∫–æ–¥–∏—Ä—É–µ–º HTML —Å—É—â–Ω–æ—Å—Ç–∏ (&nbsp; -> –ø—Ä–æ–±–µ–ª, &quot; -> " –∏ —Ç.–¥.)
    text = html.unescape(text)
    
    # 2. –£–¥–∞–ª—è–µ–º HTML —Ç–µ–≥–∏
    text = re.sub(r'<[^>]+>', ' ', text)
    
    # 3. –ò—Å–ø—Ä–∞–≤–ª—è–µ–º —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª—ã, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥–ª–∏ –æ—Å—Ç–∞—Ç—å—Å—è
    text = text.replace('\xa0', ' ') # –ù–µ—Ä–∞–∑—Ä—ã–≤–Ω—ã–π –ø—Ä–æ–±–µ–ª
    text = text.replace('&nbsp;', ' ')
    
    # 4. –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø–µ—Ä–µ–Ω–æ—Å—ã
    text = re.sub(r'\s+', ' ', text).strip()
    
    return text

def get_full_text(entry):
    """–ü—ã—Ç–∞–µ—Ç—Å—è –Ω–∞–π—Ç–∏ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –≤ RSS"""
    content = ''
    
    # 1. –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–ª–µ fulltext (–∏–Ω–æ–≥–¥–∞ –±—ã–≤–∞–µ—Ç)
    if hasattr(entry, 'content'):
        # –û–±—ã—á–Ω–æ content - —ç—Ç–æ —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π {'type': 'text/html', 'value': '...'}
        for c in entry.content:
            if c.value and len(c.value) > len(content):
                content = c.value
    
    # 2. –ï—Å–ª–∏ content –ø—É—Å—Ç, –±–µ—Ä–µ–º summary
    if not content and hasattr(entry, 'summary'):
        content = entry.summary
        
    # 3. –ï—Å–ª–∏ –≤—Å–µ –ø—É—Å—Ç–æ, –±–µ—Ä–µ–º description
    if not content and hasattr(entry, 'description'):
        content = entry.description
        
    return clean_html(content)

def parse_rss_feed(source):
    news_list = []
    print(f"üì• {source['name']}...", end=' ')
    
    try:
        d = feedparser.parse(source['url'], request_headers=HEADERS)
        
        if not d.entries:
            print("‚ùå –ü—É—Å—Ç–æ")
            return news_list

        count = 0
        for entry in d.entries[:MAX_NEWS_PER_SOURCE]:
            try:
                title = clean_html(entry.get('title', ''))
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ–≤—É—é —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø–æ–ª–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
                summary = get_full_text(entry)
                
                # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è
                if not title or not is_technical(title, summary):
                    continue
                
                # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É —Ç–µ–∫—Å—Ç–∞, —á—Ç–æ–±—ã JSON –Ω–µ —Ä–∞–∑–¥—É–≤–∞–ª—Å—è –¥–æ –≥–∏–≥–∞–±–∞–π—Ç–æ–≤,
                # –Ω–æ –¥–µ–ª–∞–µ–º –ª–∏–º–∏—Ç –±–æ–ª—å—à–∏–º (1500 —Å–∏–º–≤–æ–ª–æ–≤), —á—Ç–æ–±—ã –≤–ª–µ–∑–ª–∞ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è.
                if len(summary) > 1500:
                    summary = summary[:1500].rsplit(' ', 1)[0] + '... (–ß–∏—Ç–∞—Ç—å –¥–∞–ª–µ–µ –≤ –∏—Å—Ç–æ—á–Ω–∏–∫–µ)'

                item = {
                    'title': title,
                    'summary': summary,
                    'link': entry.get('link', ''),
                    'source': source['name'],
                    'category': source['category'],
                    'symptoms': extract_symptoms(title, summary),
                    'image': extract_image(entry),
                    'published': entry.get('published', datetime.now().isoformat())
                }
                news_list.append(item)
                count += 1
            except Exception as e:
                continue
        
        print(f"‚úÖ {count}")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
    
    return news_list

def main():
    print(f"üöÄ –ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–µ—Ä–∞... –ò—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {len(NEWS_SOURCES)}")
    all_news = []
    
    for source in NEWS_SOURCES:
        all_news.extend(parse_rss_feed(source))
    
    # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –¥–∞—Ç–µ
    all_news.sort(key=lambda x: x.get('published', ''), reverse=True)
    all_news = all_news[:MAX_TOTAL_NEWS]
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    try:
        os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)
        output = {
            'news': all_news,
            'lastUpdated': datetime.now().isoformat(),
            'totalItems': len(all_news)
        }
        with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
            json.dump(output, f, ensure_ascii=False, indent=2)
        print(f"\nüíæ –ë–∞–∑–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞! –í—Å–µ–≥–æ —Å—Ç–∞—Ç–µ–π: {len(all_news)}")
        return True
    except Exception as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {e}")
        return False

if __name__ == '__main__':
    sys.exit(0 if main() else 1)
