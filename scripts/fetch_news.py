#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import feedparser
import json
import sys
import os
import re
from datetime import datetime

# –ü–æ–¥–∫–ª—é—á–∞–µ–º –∫–æ–Ω—Ñ–∏–≥
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
from config import (
    NEWS_SOURCES, MAX_NEWS_PER_SOURCE, MAX_TOTAL_NEWS,
    EXCLUDE_KEYWORDS, TECH_KEYWORDS, SYMPTOMS_KEYWORDS, OUTPUT_FILE
)

# === –ú–ê–°–ö–ò–†–û–í–ö–ê –ü–û–î –ë–†–ê–£–ó–ï–† ===
# –≠—Ç–æ –ø–æ–º–æ–∂–µ—Ç –∏–∑–±–µ–∂–∞—Ç—å –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫ —Å–æ —Å—Ç–æ—Ä–æ–Ω—ã Drive2 –∏ –¥—Ä—É–≥–∏—Ö —Å–∞–π—Ç–æ–≤
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'
}

def is_technical(title, summary):
    text = (title + " " + summary).lower()
    # –ï—Å–ª–∏ –µ—Å—Ç—å —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ - —Å—Ä–∞–∑—É –Ω–µ—Ç
    for keyword in EXCLUDE_KEYWORDS:
        if keyword.lower() in text:
            return False
    # –ï—Å–ª–∏ –µ—Å—Ç—å —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Å–ª–æ–≤–∞ - –¥–∞
    for keyword in TECH_KEYWORDS:
        if keyword.lower() in text:
            return True
    return False

def extract_symptoms(title, summary):
    text = (title + " " + summary).lower()
    found = [s for s in SYMPTOMS_KEYWORDS if s.lower() in text]
    return list(set(found))[:8]

def extract_image(entry):
    # YouTube
    if 'youtube.com' in entry.get('link', ''):
        # –ü—ã—Ç–∞–µ–º—Å—è –¥–æ—Å—Ç–∞—Ç—å –ø—Ä–µ–≤—å—é YouTube
        if 'media_group' in entry and 'media_thumbnail' in entry.media_group[0]:
            return entry.media_group[0]['media_thumbnail'][0]['url']
    
    # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π RSS
    if hasattr(entry, 'media_content') and entry.media_content:
        return entry.media_content[0].get('url', '')
    if hasattr(entry, 'media_thumbnail') and entry.media_thumbnail:
        return entry.media_thumbnail[0].get('url', '')
    
    # –ü–æ–∏—Å–∫ –≤ HTML
    if 'summary' in entry:
        match = re.search(r'<img[^>]*src=["\'](.*?)["\']', entry.summary)
        if match:
            return match.group(1)
    return None

def clean_html(text):
    if not text: return ''
    text = re.sub(r'<[^>]+>', '', text) # –£–¥–∞–ª—è–µ–º —Ç–µ–≥–∏
    text = re.sub(r'\s+', ' ', text).strip()
    return text[:600] # –û—Å—Ç–∞–≤–ª—è–µ–º –ø–æ–±–æ–ª—å—à–µ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π

def parse_rss_feed(source):
    news_list = []
    print(f"üì• {source['name']}...", end=' ')
    
    try:
        # –ü–µ—Ä–µ–¥–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ (—á–µ—Ä–µ–∑ feedparser —ç—Ç–æ –¥–µ–ª–∞–µ—Ç—Å—è –Ω–µ—è–≤–Ω–æ, –Ω–æ –∏–Ω–æ–≥–¥–∞ –ø–æ–º–æ–≥–∞–µ—Ç request_headers)
        # –î–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π feedparser, –æ–Ω –æ–±—ã—á–Ω–æ —Å–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è —Å RSS
        d = feedparser.parse(source['url'], request_headers=HEADERS)
        
        if d.bozo and d.bozo_exception:
             print(f"‚ö†Ô∏è (XML Warning)", end=' ')

        if not d.entries:
            print("‚ùå –ü—É—Å—Ç–æ")
            return news_list

        count = 0
        for entry in d.entries[:MAX_NEWS_PER_SOURCE]:
            try:
                title = clean_html(entry.get('title', ''))
                summary = clean_html(entry.get('summary', ''))
                
                # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è
                if not title or not is_technical(title, summary):
                    continue

                item = {
                    'title': title,
                    'summary': summary,
                    'link': entry.get('link', ''),
                    'source': source['name'],
                    'category': source['category'],
                    'symptoms': extract_symptoms(title, summary),
                    'image': extract_image(entry),
                    'published': entry.get('published', datetime.now().isoformat())
                }
                news_list.append(item)
                count += 1
            except:
                continue
        
        print(f"‚úÖ {count}")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
    
    return news_list

def main():
    print(f"üöÄ –ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–µ—Ä–∞... –ò—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {len(NEWS_SOURCES)}")
    all_news = []
    
    for source in NEWS_SOURCES:
        all_news.extend(parse_rss_feed(source))
    
    # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –¥–∞—Ç–µ (—Å–≤–µ–∂–∏–µ —Å–≤–µ—Ä—Ö—É)
    all_news.sort(key=lambda x: x.get('published', ''), reverse=True)
    all_news = all_news[:MAX_TOTAL_NEWS]
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    try:
        os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)
        output = {
            'news': all_news,
            'lastUpdated': datetime.now().isoformat(),
            'totalItems': len(all_news)
        }
        with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
            json.dump(output, f, ensure_ascii=False, indent=2)
        print(f"\nüíæ –ë–∞–∑–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞! –í—Å–µ–≥–æ —Å—Ç–∞—Ç–µ–π: {len(all_news)}")
        return True
    except Exception as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {e}")
        return False

if __name__ == '__main__':
    sys.exit(0 if main() else 1)
