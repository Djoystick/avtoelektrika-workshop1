#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
üîß –ü–ê–†–°–ï–† –ú–ê–°–¢–ï–†–°–ö–û–ô –ê–í–¢–û–≠–õ–ï–ö–¢–†–ò–ö–ò v3.0
–í—ã—Ç—è–≥–∏–≤–∞–µ—Ç —Ç–æ–ª—å–∫–æ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –∏ —Ä–µ—à–µ–Ω–∏—è, –Ω–µ –Ω–æ–≤–æ—Å—Ç–∏
"""

import feedparser
import json
import sys
import os
import re
import html
from datetime import datetime
from collections import defaultdict

sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
from config import (
    NEWS_SOURCES, MAX_NEWS_PER_SOURCE, MAX_TOTAL_NEWS,
    EXCLUDE_KEYWORDS, INSTRUCTION_KEYWORDS, ERROR_CODES, PROBLEM_CATEGORIES,
    OUTPUT_FILE
)

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
}

print("\n" + "="*80)
print("üîß –ú–ê–°–¢–ï–†–°–ö–ê–Ø –ê–í–¢–û–≠–õ–ï–ö–¢–†–ò–ö–ê v3.0 - –ü–∞—Ä—Å–µ—Ä —Ä–µ—à–µ–Ω–∏–π")
print("="*80 + "\n")

# ==================================================
# –§–ò–õ–¨–¢–†–´ –ö–û–ù–¢–ï–ù–¢–ê
# ==================================================

def is_instruction_not_news(title, summary, source_name):
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç True —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —ç—Ç–æ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è/—Ä–µ—à–µ–Ω–∏–µ, –∞ –Ω–µ –Ω–æ–≤–æ—Å—Ç—å.
    –õ–æ–≥–∏–∫–∞: 
    1. –ò–°–ö–õ–Æ–ß–ò–¢–¨ –µ—Å–ª–∏ –µ—Å—Ç—å –±–∞–Ω-—Å–ª–æ–≤–∞
    2. –¢–†–ï–ë–û–í–ê–¢–¨ –µ—Å–ª–∏ –µ—Å—Ç—å —Å–ª–æ–≤–∞ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏
    """
    text = (title + " " + summary).lower()
    
    # 1. –ñ–ï–°–¢–ö–ò–ô –ë–ê–ù
    for ban_word in EXCLUDE_KEYWORDS:
        if ban_word in text:
            return False
    
    # 2. –ï—Å–ª–∏ —ç—Ç–æ —Å Drive2 –∏–ª–∏ YouTube - –±–µ—Ä–µ–º —Å–º–µ–ª–µ–µ
    if "Drive2" in source_name or "YouTube" in source_name:
        # –•–æ—Ç—è –±—ã –æ–¥–Ω–æ —Å–ª–æ–≤–æ –ø—Ä–æ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—é
        return any(kw in text for kw in INSTRUCTION_KEYWORDS)
    
    # 3. –î–ª—è —Ç–µ—Ö–ø–æ—Ä—Ç–∞–ª–æ–≤ - —Å—Ç—Ä–æ–≥–æ–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–µ
    return any(kw in text for kw in INSTRUCTION_KEYWORDS)

def extract_error_codes(text):
    """–í—ã—Ç—è–≥–∏–≤–∞–µ—Ç –∫–æ–¥—ã –æ—à–∏–±–æ–∫ –∏–∑ —Ç–µ–∫—Å—Ç–∞ (P0300, C0001 –∏ —Ç.–¥.)"""
    codes = []
    for code in ERROR_CODES:
        if code in text.upper():
            codes.append(code)
    return codes

def tag_by_problem(title, summary):
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, –∫ –∫–∞–∫–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –ø—Ä–æ–±–ª–µ–º—ã –æ—Ç–Ω–æ—Å–∏—Ç—Å—è —Å—Ç–∞—Ç—å—è"""
    text = (title + " " + summary).lower()
    
    matched_categories = []
    for category, keywords in PROBLEM_CATEGORIES.items():
        if any(kw in text for kw in keywords):
            matched_categories.append(category)
    
    return matched_categories if matched_categories else ["üìö –°–ø—Ä–∞–≤–∫–∞"]

def extract_content_type(source_name):
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é –∏—Å—Ç–æ—á–Ω–∏–∫–∞"""
    if "YouTube" in source_name:
        return "üé¨ –í–∏–¥–µ–æ"
    elif "Drive2" in source_name:
        return "üí¨ –§–æ—Ä—É–º"
    elif "–õ–∞–¥–∞" in source_name or "ABW" in source_name:
        return "üìñ –°–ø—Ä–∞–≤–∫–∞"
    return "üìö –°—Ç–∞—Ç—å—è"

def clean_html(text):
    """–û—á–∏—â–∞–µ—Ç —Ç–µ–∫—Å—Ç –æ—Ç HTML-–º—É—Å–æ—Ä–∞"""
    if not text: return ''
    text = html.unescape(text)
    text = re.sub(r'<[^>]+>', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def get_best_text(entry):
    """–í—ã—Ç—è–≥–∏–≤–∞–µ—Ç –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –¥–ª–∏–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç"""
    candidates = []
    if hasattr(entry, 'content'):
        for c in entry.content:
            if c.value:
                candidates.append(c.value)
    if hasattr(entry, 'summary') and entry.summary:
        candidates.append(entry.summary)
    if hasattr(entry, 'description') and entry.description:
        candidates.append(entry.description)
    
    # –ë–µ—Ä–µ–º —Å–∞–º—ã–π –¥–ª–∏–Ω–Ω—ã–π
    best = max(candidates, key=len) if candidates else ''
    return clean_html(best)[:2500]

def extract_image(entry):
    """–í—ã—Ç—è–≥–∏–≤–∞–µ—Ç –∫–∞—Ä—Ç–∏–Ω–∫—É"""
    if 'youtube.com' in entry.get('link', ''):
        if 'media_group' in entry and 'media_thumbnail' in entry.media_group[0]:
            return entry.media_group[0]['media_thumbnail'][0]['url']
    
    if hasattr(entry, 'enclosures'):
        for enc in entry.enclosures:
            if enc.type.startswith('image/'):
                return enc.href
    
    content = get_best_text(entry)
    match = re.search(r'<img[^>]*src=["\'](.*?)["\']', content)
    if match: return match.group(1)
    return None

# ==================================================
# –ü–ê–†–°–ò–ù–ì –ò–°–¢–û–ß–ù–ò–ö–û–í
# ==================================================

def parse_rss_source(source):
    """–ü–∞—Ä—Å–∏—Ç –æ–¥–∏–Ω RSS –∏—Å—Ç–æ—á–Ω–∏–∫"""
    results = []
    source_name = source['name']
    
    print(f"üì• {source_name[:50]:<50}", end=' ', flush=True)
    
    try:
        feed = feedparser.parse(source['url'], request_headers=HEADERS)
        
        if not feed.entries:
            print("‚ö†Ô∏è  –ü—É—Å—Ç–æ")
            return results
        
        valid_count = 0
        for entry in feed.entries[:MAX_NEWS_PER_SOURCE]:
            try:
                title = clean_html(entry.get('title', ''))
                summary = get_best_text(entry)
                
                if not title:
                    continue
                
                # –ì–õ–ê–í–ù–´–ô –§–ò–õ–¨–¢–†
                if not is_instruction_not_news(title, summary, source_name):
                    continue
                
                # –ï—Å–ª–∏ –ø—Ä–æ—à–ª–∏ —Ñ–∏–ª—å—Ç—Ä ‚Äî –±–µ—Ä–µ–º!
                error_codes = extract_error_codes(title + " " + summary)
                problem_tags = tag_by_problem(title, summary)
                content_type = extract_content_type(source_name)
                
                article = {
                    'title': title,
                    'summary': summary,
                    'link': entry.get('link', ''),
                    'source': source_name,
                    'sourceType': source.get('type', 'unknown'),
                    'category': source['category'],
                    'contentType': content_type,
                    'problemTags': problem_tags,
                    'errorCodes': error_codes,
                    'image': extract_image(entry),
                    'published': entry.get('published', datetime.now().isoformat())
                }
                
                results.append(article)
                valid_count += 1
                
            except Exception as e:
                continue
        
        print(f"‚úÖ {valid_count} –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {str(e)[:50]}")
    
    return results

# ==================================================
# –ì–õ–ê–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø
# ==================================================

def main():
    all_articles = []
    
    print(f"–ü–∞—Ä—Å–∏–Ω–≥ {len(NEWS_SOURCES)} –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤...\n")
    
    for source in NEWS_SOURCES:
        articles = parse_rss_source(source)
        all_articles.extend(articles)
    
    # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –¥–∞—Ç–µ
    all_articles.sort(key=lambda x: x.get('published', ''), reverse=True)
    all_articles = all_articles[:MAX_TOTAL_NEWS]
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
    stats = {
        'totalArticles': len(all_articles),
        'totalSources': len(set(a['source'] for a in all_articles)),
        'contentTypes': dict(sorted(
            [(ct, sum(1 for a in all_articles if a['contentType'] == ct))
             for ct in set(a['contentType'] for a in all_articles)]
        )),
        'topProblemTags': dict(sorted(
            [(tag, sum(tag in a.get('problemTags', []) for a in all_articles))
             for a in all_articles 
             for tag in a.get('problemTags', [])],
            key=lambda x: x[1],
            reverse=True
        )[:10]),
    }
    
    print(f"\n{'='*80}")
    print(f"üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê")
    print(f"{'='*80}")
    print(f"‚úÖ –í—Å–µ–≥–æ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π: {stats['totalArticles']}")
    print(f"üì° –ê–∫—Ç–∏–≤–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {stats['totalSources']}")
    print(f"\nüì∫ –¢–∏–ø—ã –∫–æ–Ω—Ç–µ–Ω—Ç–∞:")
    for ct, count in stats['contentTypes'].items():
        print(f"   {ct}: {count}")
    print(f"\nüè∑Ô∏è  –¢–æ–ø –ø—Ä–æ–±–ª–µ–º—ã:")
    for tag, count in list(stats['topProblemTags'].items())[:5]:
        print(f"   {tag}: {count}")
    print(f"{'='*80}\n")
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    try:
        os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)
        
        output = {
            'articles': all_articles,
            'stats': stats,
            'lastUpdated': datetime.now().isoformat(),
            'version': '3.0'
        }
        
        with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
            json.dump(output, f, ensure_ascii=False, indent=2)
        
        print(f"üíæ –ë–∞–∑–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {OUTPUT_FILE}")
        print(f"   –†–∞–∑–º–µ—Ä: {os.path.getsize(OUTPUT_FILE) / 1024:.1f} KB\n")
        
        return True
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {e}\n")
        return False

if __name__ == '__main__':
    success = main()
    sys.exit(0 if success else 1)
