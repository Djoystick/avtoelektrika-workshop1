#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
üí¨ Forums HTML Parser v1.0 - –í—ã—Ç—è–≥–∏–≤–∞–µ—Ç –≤–æ–ø—Ä–æ—Å—ã –∏ –æ—Ç–≤–µ—Ç—ã —á–µ—Ä–µ–∑ HTML –ø–∞—Ä—Å–∏–Ω–≥
"""

import json
import os
from datetime import datetime
from urllib.request import urlopen
from urllib.parse import urljoin
import re

try:
    from bs4 import BeautifulSoup
    HAS_BS4 = True
except ImportError:
    HAS_BS4 = False

def parse_2carpros():
    """–ü–∞—Ä—Å–∏—Ç 2CarPros.com (–ø—Ä–æ—Å—Ç–æ–π –ø–∞—Ä—Å–∏–Ω–≥)"""
    posts = []
    
    if not HAS_BS4:
        return posts
    
    try:
        url = "https://www.2carpros.com/questions/"
        response = urlopen(url, timeout=10)
        soup = BeautifulSoup(response.read(), 'html.parser')
        
        # –ò—â–µ–º –≤–æ–ø—Ä–æ—Å—ã –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
        questions = soup.find_all('div', class_='question-item')
        
        for q in questions[:20]:
            try:
                title_elem = q.find('a', class_='question-title')
                if not title_elem:
                    continue
                
                title = title_elem.get_text(strip=True)
                link = urljoin(url, title_elem.get('href', ''))
                summary = q.find('p', class_='question-snippet')
                summary_text = summary.get_text(strip=True)[:300] if summary else ""
                
                post = {
                    "id": f"forum_2carpros_{len(posts)}",
                    "title": title,
                    "summary": summary_text,
                    "link": link,
                    "source": "2CarPros.com",
                    "sourceType": "forum",
                    "contentType": "üí¨ –§–æ—Ä—É–º",
                    "category": "üîß 2CarPros",
                    "published": datetime.now().isoformat(),
                    "image": None,
                    "type": "forum_html",
                    "language": "en"
                }
                posts.append(post)
            except:
                continue
        
        return posts
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ 2CarPros: {e}")
        return []

def main():
    all_posts = []
    
    print("üí¨ Forums HTML Parser v1.0\n")
    
    if not HAS_BS4:
        print("‚ö†Ô∏è  Beautiful Soup –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω!")
        print("    GitHub Actions –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç –µ–≥–æ")
        print("    HTML –ø–∞—Ä—Å–∏–Ω–≥ –±—É–¥–µ—Ç –ø—Ä–æ–ø—É—â–µ–Ω –≤ —ç—Ç–æ–º –∑–∞–ø—É—Å–∫–µ")
    else:
        print("üì• –ü–∞—Ä—Å–∏–Ω–≥ 2CarPros.com...")
        posts = parse_2carpros()
        all_posts.extend(posts)
        if posts:
            print(f"   ‚úÖ {len(posts)} –≤–æ–ø—Ä–æ—Å–æ–≤")
        else:
            print(f"   ‚ö†Ô∏è  0 –≤–æ–ø—Ä–æ—Å–æ–≤")
    
    if not all_posts:
        print("\n‚ö†Ô∏è  HTML –ø–∞—Ä—Å–∏–Ω–≥ –Ω–µ –¥–∞–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
        return True  # –ù–µ —Å—á–∏—Ç–∞–µ–º —ç—Ç–æ –æ—à–∏–±–∫–æ–π
    
    project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    output_dir = os.path.join(project_root, "api-cache")
    output_file = os.path.join(output_dir, "forums-html.json")
    
    os.makedirs(output_dir, exist_ok=True)
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump({
            "posts": all_posts,
            "count": len(all_posts),
            "lastUpdated": datetime.now().isoformat()
        }, f, ensure_ascii=False, indent=2)
    
    print(f"\n‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {len(all_posts)} –ø–æ—Å—Ç–æ–≤ –∏–∑ HTML —Ñ–æ—Ä—É–º–æ–≤")
    return True

if __name__ == "__main__":
    import sys
    sys.exit(0 if main() else 1)
